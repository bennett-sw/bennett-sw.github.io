{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough of code\n",
    "\n",
    "The goal of this spatial model is to take data on countries' Gallup approval ratings of Russia, China, and the United States and from this back out the distance of every country (from each other) in a spatial model of preferences over countries. This is done in this .ipynb file to give a guide to the code. The code is written here in the order it appears in the original .py file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing packages and setting paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import root\n",
    "import sympy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import dual_annealing\n",
    "from scipy.optimize import differential_evolution\n",
    "import sympy\n",
    "from sympy import symbols, Matrix, sqrt, Eq, solve\n",
    "import os\n",
    "import ot  \n",
    "import scipy.integrate as integrate\n",
    "import sys\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.cluster import KMeans\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "version = 2005 \n",
    "user_path = os.path.expanduser('~')\n",
    "dropbox = os.path.join(user_path, 'Dropbox (Harvard University)/coercion_intl/Build/Output/alignment_model')\n",
    "output = os.path.join(dropbox, f'output/version_{version}')\n",
    "input = os.path.join(dropbox, f'input/version_{version}')\n",
    "pickles = os.path.join(output, 'pickles')\n",
    "results_storage = os.path.join(output, 'pairwise_wasserstein')\n",
    "\n",
    "seed_num = 42\n",
    "np.random.seed(seed_num)  # Set the seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Commands to get Russia's starting points for minimization algorithm\n",
    "- Define **[`f`](/command_references/#f)**, which is just the Euclidean distance between two 2D vectors a and b  \n",
    "- Define **[`prep_russia_data`](/command_references/#prep_russia_data)**, which gets Russia's observed distances from the other two anchor countries (USA and China) and the coordinates of those anchor countries in the relevant year  \n",
    "- Define **[`region_intersection_info_russia`](/command_references/#region_intersection_info_russia)** which categorizes how to look for initial points based on the number of intersecting regions of the circles (whose radius is given by Russia's observed distances from them) around USA and China  \n",
    "- Define **[`find_initial_point_russia`](/command_references/#find_initial_point_russia)** which calls on the previous functions to find the best initial point for Russia  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance function\n",
    "def f(a, b):\n",
    "    return sqrt(sum((a[i] - b[i])**2 for i in range(len(a))))\n",
    "\n",
    "def find_midpoint_outside_circles(circle_1, circle_2):\n",
    "    # Step 1: Define the line between the centers\n",
    "    x1, y1 = circle_1['center']\n",
    "    center1 = (x1, y1)\n",
    "    x2, y2 = circle_2['center']\n",
    "    center2 = (x2, y2)\n",
    "    radius1 = circle_1['radius']\n",
    "    radius2 = circle_2['radius']\n",
    "    \n",
    "    # Vector along the line connecting the centers\n",
    "    line_vec = np.array([x2 - x1, y2 - y1])\n",
    "    #line_length = np.linalg.norm(line_vec)\n",
    "    line_length = ((x2 - x1)**2 + (y2 - y1)**2)**.5\n",
    "    unit_vec = line_vec / line_length  # Normalize the vector\n",
    "\n",
    "    # Step 2: Find the intersection of the line with the perimeters of both circles\n",
    "    def intersection_point(center, radius, unit_vec):\n",
    "        return center + unit_vec * radius\n",
    "\n",
    "    point_on_circle1 = intersection_point(center1, radius1, unit_vec)\n",
    "    point_on_circle2 = intersection_point(center2, radius2, -unit_vec)\n",
    "\n",
    "    # Step 3: Find the midpoint of the segment outside the circles\n",
    "    midpoint = (point_on_circle1 + point_on_circle2) / 2\n",
    "\n",
    "    return midpoint\n",
    "\n",
    "# functions for russia \n",
    "\n",
    "def prep_russia_data(df):\n",
    "    # getting distances dictionary \n",
    "    observed_distances = df.loc[df['country'] == 'RUS', [f'f_{i}' for i in range(1, 3)]]\n",
    "    observed_distances.reset_index(inplace=True)\n",
    "    observed_distances = observed_distances.loc[0, [f'f_{i}' for i in range(1, 3)]]\n",
    "    distances_dictionary = {}\n",
    "    for i in range(1, 3):\n",
    "        distances_dictionary[f'f_{i}'] = observed_distances[i-1]\n",
    "    \n",
    "    # getting usa and china's coordinates \n",
    "    anchors_coords = df[['x_1', 'y_1', 'x_2', 'y_2']]\n",
    "\n",
    "    subs_dict_anchors_coords = {}\n",
    "    x_values = anchors_coords.loc[0, [f'x_{i}' for i in range(1, 3)]]\n",
    "    y_values = anchors_coords.loc[0, [f'y_{i}' for i in range(1, 3)]]\n",
    "    for i in range(1, 3):\n",
    "        subs_dict_anchors_coords[f'x_{i}'] = x_values[i-1]\n",
    "        subs_dict_anchors_coords[f'y_{i}'] = y_values[i-1]\n",
    "\n",
    "    return distances_dictionary, subs_dict_anchors_coords\n",
    "\n",
    "# now, defining functions for guessing the initial point of russia; recall that it has two cases: a) one region of intersection or b) 0 regions of intersection \n",
    "# finding number of intersection points \n",
    "def region_intersection_info_russia(center1, radius1, center2, radius2, resolution=1000):\n",
    "    # Define symbols\n",
    "    x, y = sp.symbols('x y')\n",
    "    \n",
    "    # Unpack the centers\n",
    "    x1, y1 = center1\n",
    "    x2, y2 = center2\n",
    "    circles = {'circle_1': {'center': center1, 'radius': radius1}, 'circle_2': {'center': center2, 'radius': radius2}}\n",
    "    # Define the inequalities for the regions inside each circle\n",
    "    ineq1 = (x - x1)**2 + (y - y1)**2 <= radius1**2\n",
    "    ineq2 = (x - x2)**2 + (y - y2)**2 <= radius2**2\n",
    "    \n",
    "    # Create a grid of points to check overlap\n",
    "    x_vals = np.linspace(min(x1 - radius1, x2 - radius2), max(x1 + radius1, x2 + radius2), resolution)\n",
    "    y_vals = np.linspace(min(y1 - radius1, y2 - radius2), max(y1 + radius1, y2 + radius2), resolution)\n",
    "    grid_x, grid_y = np.meshgrid(x_vals, y_vals)\n",
    "    \n",
    "    # Convert inequalities to lambda functions\n",
    "    f_ineq1 = sp.lambdify((x, y), ineq1, 'numpy')\n",
    "    f_ineq2 = sp.lambdify((x, y), ineq2, 'numpy')\n",
    "    \n",
    "    # Evaluate the inequalities on the grid\n",
    "    region1 = f_ineq1(grid_x, grid_y)\n",
    "    region2 = f_ineq2(grid_x, grid_y)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Calculate pairwise overlaps by checking if there is any overlap in the regions\n",
    "    intersections = {}\n",
    "    \n",
    "    intersections['circle1_circle2'] = {\n",
    "        'intersection_points': np.column_stack((grid_x[region1 & region2], grid_y[region1 & region2])),\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Count non-empty pairwise overlaps\n",
    "    non_empty_pairwise = sum(1 for key in intersections if intersections[key]['intersection_points'].size > 0)\n",
    "    \n",
    "    # Find the intersection of all three regions\n",
    "    common_intersection_region = region1 & region2\n",
    "    non_empty_common = np.any(common_intersection_region)\n",
    "    \n",
    "    # Get the points in the intersection\n",
    "    intersection_points = np.column_stack((grid_x[common_intersection_region], grid_y[common_intersection_region]))\n",
    "\n",
    "    solutions = []\n",
    "    # also getting the two points of intersection (if they exist)\n",
    "    if non_empty_common: \n",
    "        eq1 =  (x - x1)**2 + (y - y1)**2 - radius1**2\n",
    "        eq2 = (x - x2)**2 + (y - y2)**2 - radius2**2\n",
    "        solutions = sp.solve([eq1, eq2], (x, y))\n",
    "    return non_empty_pairwise, non_empty_common, intersection_points, intersections, solutions\n",
    "\n",
    "\n",
    "# function that calls the above functions to return the right point depending on the number of regions of intersection \n",
    "def find_initial_point_russia(anchors_dictionary, distances_dictionary):\n",
    "    # first, figure out how many regions of intersection there were: \n",
    "    # anchors dict is of form: x_i and y_i, so need to assign center_i to be x_i, y_i; they should be tuples\n",
    "    center1 = (anchors_dictionary['x_1'], anchors_dictionary['y_1'])\n",
    "    center2 = (anchors_dictionary['x_2'], anchors_dictionary['y_2'])\n",
    "    radius1 = distances_dictionary['f_1']\n",
    "    radius2 = distances_dictionary['f_2']\n",
    "    # defining 'circles' dictionary so that it's easier to get at the information of each circle \n",
    "    circles = {'circle_1': {'center': center1, 'radius': radius1}, 'circle_2': {'center': center2, 'radius': radius2}}\n",
    "\n",
    "\n",
    "\n",
    "    # finding regions of common intersection\n",
    "    pairwise_count, common_intersection, intersection_points, intersections, solutions = region_intersection_info_russia(center1, radius1, center2, radius2)\n",
    "    #print(f'The number of pairs of circles that have non-empty regions of intersection is {pairwise_count}, and it is {common_intersection} that they have a common intersection.')\n",
    "    \n",
    "    # case 2a: one pair of circles has non-empty intersection \n",
    "    if pairwise_count == 1:\n",
    "        # choose one of the two points of intersection ('solutions') that has higher y value of the two circles that is closest to the third circle's point\n",
    "        initial_point = max(solutions, key = lambda x: x[1])\n",
    "\n",
    "    # case 3: no intersections between the circles\n",
    "    elif pairwise_count == 0:\n",
    "        initial_point = find_midpoint_outside_circles(circles['circle_1'], circles['circle_2'])\n",
    "    return initial_point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Russia's optimal coordinates, using minimization algorithm\n",
    "- Define **[`estimate_russia`](/command_references/#estimate_russia)**, which implements a custom Newton–Raphson loop (up to 100 iterations) to solve for `(x,y)` by linearizing the vector of implied distances. Tracks the best iterate and returns it as a Pandas Series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def estimate_russia(anchors_dictionary, distances_dictionary, initial_guess, error_threshold=0.0001):\n",
    "    if initial_guess[1] == 0: \n",
    "            best_result = {'est_x': initial_guess[0], 'est_y': initial_guess[1]}\n",
    "    else: \n",
    "        x, y = symbols('x y')\n",
    "        q = Matrix([x, y])\n",
    "        q1 = Matrix([symbols('x_1'), symbols('y_1')])\n",
    "        q2 = Matrix([symbols('x_2'), symbols('y_2')])\n",
    "        anchors = [q1, q2]\n",
    "        anchor_coordinates = Matrix([[q1.T], [q2.T]])\n",
    "        anchor_coordinates = anchor_coordinates.subs(anchors_dictionary)\n",
    "        r = Matrix([symbols('f_1'), symbols('f_2')])\n",
    "        r = r.subs(distances_dictionary)\n",
    "\n",
    "        best_result = None\n",
    "        best_error = float('inf')\n",
    "        q0 = Matrix([symbols('x_0'), symbols('y_0')])\n",
    "        fq0 = Matrix([f(q0, anchor) for anchor in anchors])\n",
    "        fq0 = fq0.subs(anchors_dictionary)\n",
    "        f_vector = Matrix([f(q, anchor) for anchor in anchors])\n",
    "        D = f_vector.jacobian([x, y])\n",
    "        D = D.subs(anchors_dictionary)\n",
    "        #print(f'D is: {D}')\n",
    "        #print(initial_guess)\n",
    "        q0_dict = {'x_0': initial_guess[0], 'y_0': initial_guess[1]}\n",
    "        q_dict = {'x': initial_guess[0], 'y': initial_guess[1]}\n",
    "        identity_matrix = sp.eye(2)\n",
    "        max_iterations = 100\n",
    "        for iter in range(max_iterations):\n",
    "            q_initial = q0.subs(q0_dict)\n",
    "            #print(f'initial point is: {q_initial}')\n",
    "            D_eval = D.subs(q_dict)\n",
    "            #print(f'D eval: {D_eval}')\n",
    "            fq_initial = fq0.subs(q0_dict)\n",
    "            test = D_eval.T * identity_matrix.inv() * D_eval\n",
    "            #print(f'test: {test}')\n",
    "            q_n = q_initial + (D_eval.T * identity_matrix.inv() * D_eval).inv() * D_eval.T * identity_matrix.inv() * (r - fq_initial)\n",
    "            q_initial = q_n\n",
    "            q0_dict = {'x_0': q_initial[0, 0], 'y_0': q_initial[1, 0]}\n",
    "            q_dict = {'x': q_initial[0, 0], 'y': q_initial[1, 0]}\n",
    "            q_final = q_initial\n",
    "            estimated_x, estimated_y = q_final[0, 0], q_final[1, 0]\n",
    "            # Calculate implied distances\n",
    "            implied_distances = np.array([f([estimated_x, estimated_y], [anchors_dictionary[f'x_{i}'], anchors_dictionary[f'y_{i}']]) for i in range(1, 3)], dtype=float)\n",
    "            observed_distances = np.array([distances_dictionary[f'f_{i}'] for i in range(1, 3)], dtype=float)\n",
    "            \n",
    "            # Calculate error as the sum of absolute differences\n",
    "            error = np.sum(np.abs(implied_distances - observed_distances))\n",
    "            \n",
    "            if error < best_error:\n",
    "                best_error = error\n",
    "                best_result = {'est_x': estimated_x, 'est_y': estimated_y}\n",
    "\n",
    "            # Early stopping if the error is below the threshold\n",
    "            if error < error_threshold:\n",
    "                break\n",
    "\n",
    "    return pd.Series(best_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build dataset with anchors' coordinates and other countries' distances to anchors \n",
    "- Define **[`build_data`](/command_references/#build_data)**, which 1. Renames Gallup disapproval columns to `f_1,f_2,f_3`  2. Zeros out self‑disapproval for reference countries  3. Drops missing data and merges with reference coordinates\n",
    "- Importing raw gallup disapproval data about Russia, USA, and China  \n",
    "- Creating China data: create rows for China (data _from_ which is missing in Gallup raw data) by taking Australia, renaming it to China, then setting its disapproval of itself to zero and of Russia and USA to 20  \n",
    "- Defining coordinates for China and USA: set China’s coordinates to \\((0,0)\\), and set the USA's to \\((x_2,0)\\) where \\(x_2\\) is USA’s disapproval of China. Then merge these into a single dataframe  \n",
    "- Setting up Russia's data: make a dataframe that stores Russia's disapproval of China and USA as `f_1` and `f_2`, then merge this with the USA/China coordinates to create `russia_df`  \n",
    "- Estimating Russian coordinates: for each year, estimate Russia’s optimal coordinates given its distances to USA/China and the anchors’ locations; store the results in `ref_coords`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(gallup_data, anchor_coords): \n",
    "    gallup_data.rename(columns={'disapproval_china': 'f_1', 'disapproval_usa': 'f_2', 'disapproval_russia': 'f_3'}, inplace=True)\n",
    "    gallup_data.loc[gallup_data['country'] == \"USA\", 'f_2'] = 0\n",
    "    gallup_data.loc[gallup_data['country'] == \"RUS\", 'f_3'] = 0\n",
    "    # next, drop any rows that have missing data in them \n",
    "    gallup_data = gallup_data.dropna()\n",
    "    \n",
    "    # next, merge the gallup data with the coordinates of usa, russia, and china\n",
    "    final = pd.merge(gallup_data, anchor_coords, on='year')\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "# Append the Dropbox and other common path\n",
    "\n",
    "gallup_path = input + f'/gallup_disapproval.csv'\n",
    "\n",
    "gallup = pd.read_csv(gallup_path)\n",
    "gallup = gallup[['year', 'country', 'disapproval_china', 'disapproval_russia', 'disapproval_usa']]\n",
    "# adding china \n",
    "gallup_china = gallup.loc[gallup['country'] == \"AUS\"]\n",
    "gallup_china['country'] = 'CHN'\n",
    "gallup_china['disapproval_china'] = 0\n",
    "gallup_china['disapproval_russia'] = 20\n",
    "gallup_china['disapproval_usa'] = 20\n",
    "gallup = pd.concat([gallup, gallup_china])\n",
    "\n",
    "gallup.loc[gallup['country'] == \"USA\", 'disapproval_usa'] = 0\n",
    "gallup.loc[gallup['country'] == \"RUS\", 'disapproval_russia'] = 0\n",
    "\n",
    "gallup_usa = gallup.loc[gallup['country'] == 'USA']\n",
    "gallup_usa['x_2'] = gallup_usa['disapproval_china']\n",
    "gallup_usa['y_2'] = 0 \n",
    "\n",
    "gallup_usa = gallup_usa[['year', 'x_2', 'y_2']]\n",
    "\n",
    "\n",
    "gallup_china = gallup.loc[gallup['country'] == \"AUS\"]\n",
    "gallup_china['x_1'] = 0\n",
    "gallup_china['y_1'] = 0\n",
    "gallup_china = gallup_china[['year', 'x_1','y_1']]\n",
    "\n",
    "usa_china_coords = pd.merge(gallup_usa, gallup_china, on='year')\n",
    "\n",
    "# getting russia's coordinates \n",
    "gallup_russia = gallup.loc[gallup['country'] == \"RUS\"]\n",
    "gallup_russia['f_1'] = gallup_russia['disapproval_china']\n",
    "gallup_russia['f_2'] = gallup_russia['disapproval_usa']\n",
    "gallup_russia = gallup_russia[['year', 'country', 'f_1', 'f_2']]\n",
    "\n",
    "russia_df = pd.merge(gallup_russia, usa_china_coords, on='year')\n",
    "\n",
    "ref_coords = russia_df[['year','x_1','y_1','x_2','y_2']]\n",
    "ref_coords['x_3'] = np.nan\n",
    "ref_coords['y_3'] = np.nan\n",
    "for annum in russia_df['year'].unique():\n",
    "    russia_df1 = russia_df.loc[russia_df['year'] == annum].reset_index()\n",
    "    russia_distances, russia_anchor_coords = prep_russia_data(russia_df1)\n",
    "    init_point = find_initial_point_russia(russia_anchor_coords, russia_distances)\n",
    "    #print(f'Russia\\'s initial point for year {annum} is {init_point}')\n",
    "    est_coords = estimate_russia(russia_anchor_coords, russia_distances, init_point)\n",
    "    ref_coords.loc[ref_coords['year'] == annum, 'x_3'] = est_coords[0]\n",
    "    ref_coords.loc[ref_coords['year'] == annum, 'y_3'] = est_coords[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve data and create posterior\n",
    "\n",
    "- Define **[`get_distances`](/command_references/#get_distances)**, which retrieves the observed distance (i.e. gallup disapproval) of a country from an anchor country and returns a dictionary of the distances  \n",
    "- Define **[`get_data`](/command_references/#get_data)**  \n",
    "- Define **[`likelihood_function`](/command_references/#likelihood_function)**  \n",
    "- Define **[`bounded_likelihood`](/command_references/#bounded_likelihood)**, which takes the likelihood and divides by its integral over the fixed bounds to produce a normalized density  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_distances(anchor_distances_dataframe, country):\n",
    "    observed_distances = anchor_distances_dataframe.loc[anchor_distances_dataframe['country'] == f'{country}', [f'f_{i}' for i in range(1, 4)]]\n",
    "    observed_distances.reset_index(inplace=True)\n",
    "    observed_distances = observed_distances.loc[0, [f'f_{i}' for i in range(1, 4)]]\n",
    "    distances_dictionary = {}\n",
    "    for i in range(1, 4):\n",
    "        distances_dictionary[f'f_{i}'] = observed_distances[i-1]\n",
    "    converted_dict = {key: np.float64(value) for key, value in distances_dictionary.items()}\n",
    "    return converted_dict\n",
    "\n",
    "\n",
    "\n",
    "def get_data(state, time):\n",
    "#for annum in all_years:\n",
    "    for annum in [time]:\n",
    "        source = data.loc[data['year'] == annum]\n",
    "        source.reset_index(inplace=True)\n",
    "        x_values = source.loc[0, [f'x_{i}' for i in range(1, 4)]]\n",
    "        y_values = source.loc[0, [f'y_{i}' for i in range(1, 4)]]\n",
    "        subs_dict_anchors_coords = {}\n",
    "        for i in range(1, 4):\n",
    "            subs_dict_anchors_coords[f'x_{i}'] = x_values[i-1]\n",
    "            subs_dict_anchors_coords[f'y_{i}'] = y_values[i-1]\n",
    "        anchors_dict = subs_dict_anchors_coords\n",
    "        anchors_dict_floats = {key: np.float64(value) for key, value in anchors_dict.items()}\n",
    "\n",
    "        df_year = source \n",
    "        df_year['est_x'] = np.nan\n",
    "        df_year['est_y'] = np.nan\n",
    "        all_countries = df_year['country'].unique()\n",
    "        non_ref_countries = np.delete(all_countries, np.where((all_countries == \"USA\") | (all_countries == \"RUS\") | (all_countries == \"CHN\")))\n",
    "        for country in [state]: \n",
    "            #print(f'The country is {country} and the year is {annum}')\n",
    "            anchor_distances_dict = get_distances(df_year, country)\n",
    "            final = source.loc[source['country'] == country]       \n",
    "    return anchor_distances_dict, anchors_dict_floats\n",
    "\n",
    "# Function to calculate the likelihood\n",
    "def likelihood_function(x, y, observed_distances, anchor_locations):\n",
    "    \"\"\"\n",
    "    Computes the likelihood of the coordinates (x, y) given noisy distance observations from three anchors.\n",
    "    \n",
    "    Args:\n",
    "    x, y: Coordinates of the object of interest\n",
    "    observed_distances: A list or array of observed noisy distances to the anchors [d1, d2, d3]\n",
    "    anchor_locations: A list or array of known anchor locations [(x1, y1), (x2, y2), (x3, y3)]\n",
    "    sigma: Standard deviation of the noise, default set to 0.1\n",
    "    \n",
    "    Returns:\n",
    "    Likelihood value for the given (x, y)\n",
    "    \"\"\"\n",
    "    sigma = sigma_dict[annum]\n",
    "    # Extracting the anchor coordinates\n",
    "    x1, y1 = anchor_locations['x_1'], anchor_locations['y_1']\n",
    "    x2, y2 = anchor_locations['x_2'], anchor_locations['y_2']\n",
    "    x3, y3 = anchor_locations['x_3'], anchor_locations['y_3']\n",
    "    \n",
    "    # Observed distances\n",
    "    d1, d2, d3 = observed_distances['f_1'], observed_distances['f_2'], observed_distances['f_3']\n",
    "    \n",
    "    # True distances (r1, r2, r3)\n",
    "    r1 = np.sqrt((x - x1)**2 + (y - y1)**2)\n",
    "    r2 = np.sqrt((x - x2)**2 + (y - y2)**2)\n",
    "    r3 = np.sqrt((x - x3)**2 + (y - y3)**2)\n",
    "    \n",
    "    # Likelihood function assuming Gaussian noise\n",
    "    likelihood = (1 / (np.sqrt(2 * np.pi * sigma**2)))**3 * np.exp(\n",
    "        -((d1 - r1)**2 + (d2 - r2)**2 + (d3 - r3)**2) / (2 * sigma**2)\n",
    "    )\n",
    "    \n",
    "    return likelihood\n",
    "\n",
    "def int_of_likelihood(anchor_distances, anchor_placement, bounds):\n",
    "    def integrand(x, y):\n",
    "        return likelihood_function(x, y, anchor_distances, anchor_placement)\n",
    "    x_min, x_max = -1.5, 1.5\n",
    "    y_min, y_max = -1.5, 1.5\n",
    "    result, error = integrate.dblquad(integrand, x_min, x_max, lambda x: y_min, lambda x: y_max)\n",
    "    #print(f'The likelihood function integrates to {result}')\n",
    "    return result\n",
    "def bounded_likelihood(x,y, anchor_distances, anchor_placement, total): \n",
    "    bounded_likelihood = likelihood_function(x,y, anchor_distances, anchor_placement)/total\n",
    "    return bounded_likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find representative samples of posterior distribution\n",
    "- Define **[`quantize_distribution_kmeans`](/command_references/#quantize_distribution_kmeans)**: see [this link`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). We collapse a large (100k) 2D sample into a smaller (1k) representative set by KMeans, and define the distribution to have points at the center of each cluster and weight each center by the number of points (from the 100k sample) in that cluster. \n",
    "- Create **[`class LikelihoodEstimator`](/command_references/#class-likelihoodestimator)** which stores the posterior so we only load the anchor data (and therefore the relevant posterior) once per country‑year.\n",
    "- Define **[`empirical_distribution`](/command_references/#empirical_distribution)**: returns a discretized distribution of 1,000 points (and weights) using kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def quantize_distribution_kmeans(samples, n_clusters, random_state=seed_num):\n",
    "    \"\"\"\n",
    "    Quantizes a 2D distribution by clustering with KMeans.\n",
    "\n",
    "    Args:\n",
    "        samples: np.ndarray of shape (n_samples, 2)\n",
    "        n_clusters: int, number of centroids to find\n",
    "        random_state: int, for reproducibility of KMeans\n",
    "\n",
    "    Returns:\n",
    "        centers: np.ndarray of shape (n_clusters, 2), the cluster centers\n",
    "        weights: np.ndarray of shape (n_clusters,), summing to 1,\n",
    "                 the proportion of samples in each cluster\n",
    "    \"\"\"\n",
    "    # 1) Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    labels = kmeans.fit_predict(samples)    # shape (n_samples,)\n",
    "    centers = kmeans.cluster_centers_      # shape (n_clusters, 2)\n",
    "\n",
    "    # 2) Count points per cluster\n",
    "    counts = np.bincount(labels, minlength=n_clusters)  # shape (n_clusters,)\n",
    "\n",
    "    # 3) Convert counts → weights summing to 1\n",
    "    weights = counts.astype(float) / counts.sum()\n",
    "\n",
    "    return centers, weights\n",
    "class LikelihoodEstimator:\n",
    "    \"\"\"\n",
    "    Wraps the original likelihood_function to load anchor data once.\n",
    "    \"\"\"\n",
    "    def __init__(self, anchor_distances, anchor_placement):\n",
    "        self.observed = anchor_distances\n",
    "        self.anchors = anchor_placement\n",
    "\n",
    "    def pdf(self, x, y):\n",
    "        # Exactly the same math as likelihood_function\n",
    "        sigma = sigma_dict[annum]\n",
    "        x1, y1 = self.anchors['x_1'], self.anchors['y_1']\n",
    "        x2, y2 = self.anchors['x_2'], self.anchors['y_2']\n",
    "        x3, y3 = self.anchors['x_3'], self.anchors['y_3']\n",
    "        d1, d2, d3 = self.observed['f_1'], self.observed['f_2'], self.observed['f_3']\n",
    "        r1 = np.sqrt((x - x1)**2 + (y - y1)**2)\n",
    "        r2 = np.sqrt((x - x2)**2 + (y - y2)**2)\n",
    "        r3 = np.sqrt((x - x3)**2 + (y - y3)**2)\n",
    "        norm = (1 / np.sqrt(2 * np.pi * sigma**2))**3\n",
    "        exponent = -((d1 - r1)**2 + (d2 - r2)**2 + (d3 - r3)**2) / (2 * sigma**2)\n",
    "        return norm * np.exp(exponent)\n",
    "\n",
    "\n",
    "def empirical_distribution(country, year, sample_size):\n",
    "    # 1) load anchor data\n",
    "    anchor_distances, anchor_placement = get_data(country, year)\n",
    "    le = LikelihoodEstimator(anchor_distances, anchor_placement)\n",
    "\n",
    "    bounds = [[-1.5,1.5], [-1.5,1.5]]\n",
    "\n",
    "    # 2) integrate and find max exactly as before\n",
    "    integral = int_of_likelihood(anchor_distances, anchor_placement, bounds)\n",
    "    #print(f'The integral is {integral}')\n",
    "    def custom_pdf(x, y):\n",
    "        return le.pdf(x, y) / integral\n",
    "    def neg_bounded_likelihood(xy):\n",
    "        return -custom_pdf(xy[0], xy[1])\n",
    "    result = differential_evolution(neg_bounded_likelihood, bounds)\n",
    "    M = -result.fun\n",
    "    #print(f'Max of the likelihood is {M}')\n",
    "\n",
    "    # 3) batch rejection sampling\n",
    "    def rejection_sampling_batch(pdf, bounds, M, num_samples, batch=10000):\n",
    "        (x0, x1), (y0, y1) = bounds\n",
    "        samples = []\n",
    "        while len(samples) < num_samples:\n",
    "            X = np.random.uniform(x0, x1, size=batch)\n",
    "            Y = np.random.uniform(y0, y1, size=batch)\n",
    "            U = np.random.uniform(0, M, size=batch)\n",
    "            mask = U <= pdf(X, Y)\n",
    "            if mask.any():\n",
    "                pts = np.column_stack([X[mask], Y[mask]])\n",
    "                samples.append(pts)\n",
    "        all_samps = np.vstack(samples)[:num_samples]\n",
    "        return all_samps\n",
    "\n",
    "    if country not in ['RUS', 'CHN', 'USA']:\n",
    "        # original quantization & weighting kept\n",
    "        samples = rejection_sampling_batch(custom_pdf, bounds, M, sample_size)\n",
    "        samples_quantized, marginal_weighted = quantize_distribution_kmeans(samples, 1000)\n",
    "    else:\n",
    "        # reference points unchanged\n",
    "        if country == 'RUS':\n",
    "            coord = (anchor_placement['x_3'], anchor_placement['y_3'])\n",
    "        elif country == 'USA':\n",
    "            coord = (anchor_placement['x_2'], anchor_placement['y_2'])\n",
    "        else:\n",
    "            coord = (anchor_placement['x_1'], anchor_placement['y_1'])\n",
    "        samples_quantized = np.array([coord])\n",
    "        marginal_weighted = [1]\n",
    "\n",
    "    return samples_quantized, marginal_weighted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve data and store posterior distribution\n",
    "- **[`get_info_for_row`](/command_references/#get_info_for_row)** writes each country-year’s empirical distribution to a pickle  \n",
    "- **[`get_info_for_all_rows`](/command_references/#get_info_for_all_rows)** parallelizes the above  \n",
    "- **[`load_pickled_objects`](/command_references/#load_pickled_objects)** loads two pickles for a given country-year pair  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_info_for_row(task):\n",
    "    country, year = task\n",
    "    samples, marginals = empirical_distribution(country, year, 100000)\n",
    "    pdf_dict = {'samples': samples, 'marginals': marginals}\n",
    "    file = os.path.join(pickles, f'{country}_{year}_data.pkl')\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(pdf_dict, f)\n",
    "\n",
    "def get_info_for_all_rows(df, n_processes=7):\n",
    "    # Build a list of simple tuples instead of full Series\n",
    "    tasks = list(df[['country','year']].itertuples(index=False, name=None))\n",
    "    with Pool(processes=n_processes) as pool:\n",
    "        pool.map(get_info_for_row, tasks)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_pickled_objects(directory, country1, country2, year, method):\n",
    "    if method == 1:\n",
    "        method = ''\n",
    "    data_dict = {}\n",
    "    # Load the pickled object\n",
    "    filepath1 = f'{directory}/{country1}_{year}_data{method}.pkl'\n",
    "    with open(filepath1, 'rb') as file:\n",
    "        obj = pickle.load(file)\n",
    "        \n",
    "        # Use the filename (without .pkl) as the dictionary key\n",
    "        key = f'{country1}-{year}'\n",
    "        data_dict[key] = obj\n",
    "    filepath2 = f'{directory}/{country2}_{year}_data{method}.pkl'\n",
    "    with open(filepath2, 'rb') as file:\n",
    "        obj = pickle.load(file)\n",
    "        \n",
    "        # Use the filename (without .pkl) as the dictionary key\n",
    "        key = f'{country2}-{year}'\n",
    "        data_dict[key] = obj\n",
    "    return data_dict    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Wasserstein distance between two countries' posterior distributions\n",
    "- Define **[`wasserstein_pot_weighted`](/command_references/#wasserstein_pot_weighted)**, which computes the \\(W_\\ell\\) distance between two weighted samples via POT’s `emd2`  \n",
    "- Define **[`compute_wasserstein_distances`](/command_references/#compute_wasserstein_distances)**, which fills a symmetric matrix of pairwise Wasserstein distances for all country-years in a dataframe  \n",
    "- Define **[`unpack_distance_matrix`](/command_references/#unpack_distance_matrix)**, which converts the upper triangle of that matrix into a long-form country1-country2-year table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wasserstein_pot_weighted(sample1, sample2, pdf_sample1, pdf_sample2, l):\n",
    "    # Normalize the PDFs to get weights\n",
    "    a = pdf_sample1/ np.sum(pdf_sample1)   # Weights for sample1\n",
    "    b = pdf_sample2/ np.sum(pdf_sample2) # Weights for sample2\n",
    "    M = ot.dist(sample1, sample2, metric= 'minkowski', p = l)\n",
    "    # Compute the Wasserstein distance using POT\n",
    "    res = (ot.emd2(a, b, M,numItermax=300000, numThreads=7))**(1/l)\n",
    "    return res\n",
    "\n",
    "def compute_wasserstein_distances(df, p, method):\n",
    "    \"\"\"\n",
    "    df: contains only one unique year.\n",
    "    \"\"\"\n",
    "    num = len(df)\n",
    "    distance_matrix = np.zeros((num, num))\n",
    "    year = df.iloc[0]['year']\n",
    "    \n",
    "    # Load all pdf pickles for this year once\n",
    "    suffix = '' if method == 1 else method\n",
    "    pattern = os.path.join(pickles, f\"*_{year}_data{suffix}.pkl\")\n",
    "    pdfs = {}\n",
    "    for filepath in glob(pattern):\n",
    "        country = Path(filepath).stem.split('_')[0]\n",
    "        with open(filepath, 'rb') as f:\n",
    "            pdfs[country] = pickle.load(f)\n",
    "    \n",
    "    # Now fill the matrix without further disk I/O\n",
    "    for i in range(num):\n",
    "        ci, yi = df.iloc[i]['country'], df.iloc[i]['year']\n",
    "        pdf_i = pdfs.get(ci)\n",
    "        if pdf_i is None: \n",
    "            distance_matrix[i, i:] = np.nan\n",
    "            continue\n",
    "        \n",
    "        for j in range(i, num):\n",
    "            cj = df.iloc[j]['country']\n",
    "            pdf_j = pdfs.get(cj)\n",
    "            if pdf_j is None:\n",
    "                distance_matrix[i, j] = distance_matrix[j, i] = np.nan\n",
    "            else:\n",
    "                s_i, m_i = pdf_i['samples'], pdf_i['marginals']\n",
    "                s_j, m_j = pdf_j['samples'], pdf_j['marginals']\n",
    "                w = wasserstein_pot_weighted(s_i, s_j, m_i, m_j, p)\n",
    "                distance_matrix[i, j] = distance_matrix[j, i] = w\n",
    "\n",
    "    return pd.DataFrame(distance_matrix, index=df.country, columns=df.country)\n",
    "\n",
    "\n",
    "def unpack_distance_matrix(distance_df, p):\n",
    "    \"\"\"\n",
    "    Unpacks the distance matrix into a long-form dataframe with columns: 'country 1', 'country 2', 'distance'.\n",
    "    \n",
    "    Args:\n",
    "    distance_df: A square dataframe where each cell contains the Wasserstein distance between two objects.\n",
    "    \n",
    "    Returns:\n",
    "    long_form_df: A long-form dataframe with columns: 'country 1', 'country 2', 'distance'.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the rows\n",
    "    rows = []\n",
    "    \n",
    "    # Iterate over the rows and columns of the distance_df to unpack it\n",
    "    for i in range(len(distance_df)):\n",
    "        for j in range(i+1, len(distance_df)):  # Only loop over the upper triangle (i < j)\n",
    "            country1 = distance_df.index[i]\n",
    "            country2 = distance_df.columns[j]\n",
    "            distance = distance_df.iloc[i, j]\n",
    "            rows.append([country1, country2, distance])\n",
    "\n",
    "    # Create a new dataframe from the list of rows\n",
    "    long_form_df = pd.DataFrame(rows, columns=['country 1', 'country 2', f'distance_w_{p}'])\n",
    "    \n",
    "    return long_form_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in data and execute commands\n",
    "- Load optimal variance on noise (sigma) per year  \n",
    "- Build Gallup data and anchor coordinates  \n",
    "- Call **[`get_info_for_all_rows`](/command_references/#get_info_for_all_rows)** to generate pickles  \n",
    "- Loop over each year: subset data, compute/load empirical distributions, use **[`compute_wasserstein_distances`](/command_references/#compute_wasserstein_distances)**, **[`unpack_distance_matrix`](/command_references/#unpack_distance_matrix)**, and write CSVs  \n",
    "- Finally, concatenate all years’ distance tables into one CSV  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimal variance on the noise parameter: \n",
    "sigma_file = os.path.join(input, 'different_sigma/optimal_noise_by_year.csv')\n",
    "optimal_sigma_df = pd.read_csv(sigma_file)\n",
    "sigma_dict = optimal_sigma_df.set_index('year')['optimal_noise'].to_dict()\n",
    "\n",
    "\n",
    "data = build_data(gallup, ref_coords)\n",
    "subset = data\n",
    "data_for_dist_info = subset[['country', 'year']]\n",
    "if __name__ == '__main__':\n",
    "    data_for_dist_info_dict = get_info_for_all_rows(data_for_dist_info, n_processes=7)\n",
    "    year_df_list = []\n",
    "    for year in data_for_dist_info['year'].unique():\n",
    "    #for year in [2015]:\n",
    "        df = data_for_dist_info.loc[data_for_dist_info['year']==year]\n",
    "        # setting the following variable so that likelihood_function will use the right sigma \n",
    "        annum = year \n",
    "        results_df_1 = compute_wasserstein_distances(df, 2, 1)    \n",
    "        results_clean_1 = unpack_distance_matrix(results_df_1, 2)\n",
    "        results_clean_1['year'] = year\n",
    "        results_clean_1.rename(columns={'distance_w_2':'distance_w_2_1'}, inplace=True)\n",
    "        results_df = results_clean_1\n",
    "        yearly_file = os.path.join(results_storage, f'{year}.csv')\n",
    "        results_df.to_csv(yearly_file)\n",
    "        year_df_list.append(results_df)\n",
    "    year_df_list[0].head(10)\n",
    "    df_all = pd.concat(year_df_list)\n",
    "    all_years_file = os.path.join(results_storage, 'all_years.csv')\n",
    "    df_all.to_csv(all_years_file)\n",
    "    df = df_all\n",
    "    df.head(10)\n",
    "    df2 = df[['country 1', 'country 2', 'distance_w_2_1', 'year']]\n",
    "    df2 = df2.rename(columns={'country 1': 'country 2', 'country 2': 'country 1'})\n",
    "    df3 = pd.concat([df,df2], ignore_index=True)\n",
    "    final_results = os.path.join(input, 'gallup_alignment.csv')\n",
    "    df3.to_csv(final_results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
